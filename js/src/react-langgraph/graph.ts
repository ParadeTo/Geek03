import {StateGraph, Annotation} from '@langchain/langgraph'
import {AIMessage, HumanMessage, ToolMessage} from '@langchain/core/messages'
import {ChatOpenAI} from '@langchain/openai'
import {tools} from './tools'
import * as dotenv from 'dotenv'

dotenv.config()

/**
 * å®šä¹‰çŠ¶æ€ç»“æ„ï¼ˆä½¿ç”¨ LangGraph çš„ Annotationï¼‰
 */
const GraphState = Annotation.Root({
  messages: Annotation<any[]>({
    reducer: (x, y) => x.concat(y),
    default: () => [],
  }),
  output: Annotation<string>({
    reducer: (x, y) => y ?? x,
    default: () => '',
  }),
})

/**
 * åˆ›å»º LLM å®ä¾‹ï¼ˆå¸¦å·¥å…·ç»‘å®šï¼‰
 */
const llm = new ChatOpenAI({
  modelName: 'gpt-4o',
  apiKey: process.env.API_KEY || '',
  configuration: {
    baseURL: 'http://localhost:3001',
  },
}).bindTools(tools)

/**
 * LLM è°ƒç”¨èŠ‚ç‚¹
 */
async function callModel(state: typeof GraphState.State) {
  console.log('\nğŸ¤– LLM èŠ‚ç‚¹: è°ƒç”¨æ¨¡å‹...')

  const response = await llm.invoke(state.messages)

  // å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ˜¾ç¤ºä¿¡æ¯
  if (response.tool_calls && response.tool_calls.length > 0) {
    console.log('ğŸ“ æ¨¡å‹è¯·æ±‚è°ƒç”¨å·¥å…·:')
    response.tool_calls.forEach((tc: any) => {
      console.log(`   - ${tc.name}(${JSON.stringify(tc.args)})`)
    })
  }

  return {messages: [response]}
}

/**
 * å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
 */
async function callTools(state: typeof GraphState.State) {
  console.log('\nğŸ”§ å·¥å…·èŠ‚ç‚¹: æ‰§è¡Œå·¥å…·...')

  const lastMessage = state.messages[state.messages.length - 1] as AIMessage
  const toolCalls = lastMessage.tool_calls || []

  const toolMessages = await Promise.all(
    toolCalls.map(async (tc: any) => {
      const tool = tools.find((t) => t.name === tc.name)
      if (!tool) {
        return new ToolMessage({
          content: `å·¥å…· ${tc.name} ä¸å­˜åœ¨`,
          tool_call_id: tc.id!,
        })
      }

      console.log(`   æ‰§è¡Œ: ${tc.name}(${JSON.stringify(tc.args)})`)
      const result = await tool.invoke(tc.args)
      console.log(`   ç»“æœ: ${result}`)

      return new ToolMessage({
        content: String(result),
        tool_call_id: tc.id!,
      })
    })
  )

  return {messages: toolMessages}
}

/**
 * è·¯ç”±å‡½æ•°ï¼šå†³å®šä¸‹ä¸€æ­¥
 */
function shouldContinue(state: typeof GraphState.State) {
  const lastMessage = state.messages[state.messages.length - 1] as AIMessage

  // å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·
  if (lastMessage.tool_calls && lastMessage.tool_calls.length > 0) {
    return 'tools'
  }

  // å¦åˆ™ç»“æŸ
  return END
}

/**
 * æ„å»ºå¹¶ç¼–è¯‘å›¾
 */
export function buildGraph() {
  const workflow = new StateGraph(GraphState)
    .addNode('agent', callModel)
    .addNode('tools', callTools)
    .addEdge('__start__', 'agent')
    .addConditionalEdges('agent', shouldContinue, {
      tools: 'tools',
      [END]: END,
    })
    .addEdge('tools', 'agent')

  return workflow.compile()
}

/**
 * è¿è¡Œ Agent
 */
export async function runReactGraph(query: string): Promise<string> {
  console.log('\nğŸš€ å¯åŠ¨ ReAct Agent (LangGraph å®˜æ–¹åº“å®ç°)')
  console.log(`â“ ç”¨æˆ·é—®é¢˜: ${query}\n`)

  const app = buildGraph()

  const result = await app.invoke({
    messages: [new HumanMessage(query)],
  })

  const lastMessage = result.messages[result.messages.length - 1]
  const finalAnswer = lastMessage.content

  console.log('\n' + '='.repeat(50))
  console.log('âœ… å®Œæˆï¼')
  console.log('='.repeat(50))

  return finalAnswer as string
}
